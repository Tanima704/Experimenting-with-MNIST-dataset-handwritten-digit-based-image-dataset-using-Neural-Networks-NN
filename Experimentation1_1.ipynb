{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SArgW_Vq-uTh"
   },
   "source": [
    "The goal of this project is to run some experiments with scikit-learn on a fairly sizeable and interesting image data set. This is the MNIST data set that consists of lots of images, each having 28x28 pixels. By today's standards, this may seem relatively tiny, but only a few years ago was quite challenging computationally, and it motivated the development of several ML algorithms and models that are now state-of-the-art  solutions for much bigger data sets. \n",
    "\n",
    "The project is experimental. I will try to experiment whether a combination of PCA and kNN can yield any good results for the MNIST data set. Let's see if it can be made to work on this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFM4hig-uTj"
   },
   "source": [
    "## Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E3alYkjM-uTk"
   },
   "outputs": [],
   "source": [
    "# Import all necessary python packages\n",
    "import numpy as np\n",
    "#import os\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import ListedColormap\n",
    "#from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4JEBC9tZEZel"
   },
   "outputs": [],
   "source": [
    "# Load the data set directly from scikit learn \n",
    "# \n",
    "# note: this operation may take a few seconds. If for any reason it fails I \n",
    "# can revert back to loading from local storage. \n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = y.astype(int)\n",
    "X = ((X / 255.) - .5) * 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfrfDK0P-uT5"
   },
   "source": [
    "## <font color = 'blue'> Inspecting the Dataset </font>\n",
    "\n",
    "**(i)** How many data points are in the training and test sets ? <br>\n",
    "**(ii)** How many attributes does the data set have ?\n",
    "\n",
    "**(iii)** How many different labels does this data set have. Demonsrate how to read that number from the vector of labels *y_train*.  <br>\n",
    "**(iv)** How does the number of attributes relates to the size of the images? <br>\n",
    "**(v)** What is the role of line 12 in the above code? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4po5m-tq-uT6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is  (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "#(i)\n",
    "#With NumPy data structure, data elements are stored in the form of an array. \n",
    "#When the shape() method is associated with the NumPy array, \n",
    "#the dimensions of the array are represented in the form of a tuple\n",
    "\n",
    "\n",
    "print(\"x_train shape is \",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape is  (60000,)\n"
     ]
    }
   ],
   "source": [
    "#(i)\n",
    "print(\"y_train shape is \",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 60000 data points in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape is  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# (i)\n",
    "print(\"X_test shape is \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape is  (10000,)\n"
     ]
    }
   ],
   "source": [
    "#(i)\n",
    "print(\"y_test shape is \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 10000 data points in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is the description of y_train <bound method NDFrame.describe of 21107    4\n",
      "48688    8\n",
      "41009    7\n",
      "28301    8\n",
      "44449    0\n",
      "        ..\n",
      "46256    1\n",
      "3895     7\n",
      "60280    2\n",
      "47094    4\n",
      "6545     3\n",
      "Name: class, Length: 60000, dtype: int32>\n",
      "Following is the description of y_test <bound method NDFrame.describe of 17739    5\n",
      "362      9\n",
      "6726     6\n",
      "7137     5\n",
      "50611    3\n",
      "        ..\n",
      "10580    1\n",
      "67063    1\n",
      "60503    2\n",
      "61660    7\n",
      "30906    1\n",
      "Name: class, Length: 10000, dtype: int32>\n"
     ]
    }
   ],
   "source": [
    "# (ii)\n",
    "#The info () method prints information about the DataFrame. \n",
    "#The information contains the number of columns, column labels, column data types, \n",
    "#memory usage, range index, and the number of cells in each column (non-null values).\n",
    "# as y_train and y_test are series, hence info() method will not work on it. 'describe' methond will work on that.\n",
    "\n",
    "print('Following is the description of y_train',y_train.describe)\n",
    "\n",
    "print('Following is the description of y_test',y_test.describe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60000 entries, 21107 to 6545\n",
      "Columns: 784 entries, pixel1 to pixel784\n",
      "dtypes: float64(784)\n",
      "memory usage: 359.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# (ii)\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 17739 to 30906\n",
      "Columns: 784 entries, pixel1 to pixel784\n",
      "dtypes: float64(784)\n",
      "memory usage: 59.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#(ii)\n",
    "print(X_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 784 attributes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels  [4 8 7 0 5 2 6 9 3 1]\n",
      "Number of Unique labels  10\n"
     ]
    }
   ],
   "source": [
    "# (iii)\n",
    "unique_label =y_train.unique()\n",
    "print('Unique labels ', unique_label)\n",
    "print('Number of Unique labels ',len(unique_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iv)\n",
    "# Its known that , that 28x28 pixel which means its 784 pixels for each image.\n",
    "# The number of attributes also corresponds to 784 , which means, each pixel information for each image is available as \n",
    "#attributes. This is how the number of attributes relates to the size of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(v)\n",
    "#As Computers are digital machines, Images had to be broken down into pixels so that a computer \n",
    "#could represent them digitally. It isnâ€™t possible to represent all of the colors in the world, \n",
    "#because the color spectrum is continuous and computers work with discrete values.\n",
    "#Each basic color (Red, Green, and Blue) is 8 bits, so they are each limited to 256 (i.e 2^8) , \n",
    "#in this case 255 since 0 is included. Hence by dividing by 255, the 0-255 range can be described \n",
    "#with a 0.0-1.0 range where 0.0 means 0 (0x00) and 1.0 means 255 (0xFF).Hence after this Normalization , \n",
    "#getting thedifference from midpoint i.e. 0.5 and dividing that by 2 gives a standardized result.\n",
    "# This is the significance of line 12 in the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> PCA on MNIST </font>\n",
    "\n",
    "Because the number of attributes of the MNIST data set may be too big to apply kNN on it (due to the 'curse of dimensionality'), I want to compress the images down to a smaller number of 'fake' attributes. \n",
    "\n",
    "Use scikit-learn to output a data set *X_train_transformed* and *X_test_transformed*, with $l$ attributes. Here a reasonable choice of $l$ is 10, equal to the number of labels. But I can try slightly smaller or bigger values as well. \n",
    "\n",
    "This computation can take a while. If problems are encountered I can try the same experiment on a downsized data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0ek4Ry_-uT_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03437929  5.93042918 -1.32557813 ...  0.80195449  0.7978461\n",
      "   0.24621453]\n",
      " [ 1.71107107  0.61959372 -5.61617337 ... -1.90770905  4.24306747\n",
      "   0.74074022]\n",
      " [-3.70015368  6.08597826 -2.92447206 ...  3.91616948 -3.0181605\n",
      "   1.97646634]\n",
      " ...\n",
      " [-1.38016625 -7.54468152  0.50227195 ...  0.45987978  0.99481983\n",
      "   0.98419285]\n",
      " [ 1.20194888  7.38339717 -1.36960361 ... -0.36814754  4.36011049\n",
      "   2.00813116]\n",
      " [-1.08511144 -4.05124468 -6.87142856 ...  5.18837101 -1.1371051\n",
      "  -0.03534276]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sc = StandardScaler()\n",
    "#X_train_std = sc.fit_transform(X_train)\n",
    "#X_test_std = sc.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10).fit(X_train)\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "print(X_train_transformed)\n",
    "\n",
    "#plt.bar(range(1, 16), pca.explained_variance_ratio_, alpha=0.5, align='center')\n",
    "#plt.step(range(1, 16), np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "#plt.ylabel('Explained variance ratio')\n",
    "#plt.xlabel('Principal components')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03437929  5.93042918 -1.32557813 ...  0.80195449  0.7978461\n",
      "   0.24621453]\n",
      " [ 1.71107107  0.61959372 -5.61617337 ... -1.90770905  4.24306747\n",
      "   0.74074022]\n",
      " [-3.70015368  6.08597826 -2.92447206 ...  3.91616948 -3.0181605\n",
      "   1.97646634]\n",
      " ...\n",
      " [-1.38016625 -7.54468152  0.50227195 ...  0.45987978  0.99481983\n",
      "   0.98419285]\n",
      " [ 1.20194888  7.38339717 -1.36960361 ... -0.36814754  4.36011049\n",
      "   2.00813116]\n",
      " [-1.08511144 -4.05124468 -6.87142856 ...  5.18837101 -1.1371051\n",
      "  -0.03534276]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.03519093e+00 -1.98450351e+00  1.13101908e+00 ... -8.78126774e-02\n",
      "   3.01389637e+00 -1.37343158e+00]\n",
      " [-2.40049598e+00  6.22497719e+00  3.36945207e-03 ... -9.45531069e-01\n",
      "  -1.76392609e+00  1.41181420e+00]\n",
      " [ 3.00133273e+00  1.35338220e+00 -1.65933361e-01 ...  2.03894281e+00\n",
      "  -2.93231334e-01 -7.51156528e-01]\n",
      " ...\n",
      " [ 5.16858182e+00 -3.04103646e+00  2.89070906e+00 ...  5.93459701e+00\n",
      "  -4.43076191e+00 -1.31899591e+00]\n",
      " [-5.94360770e+00 -1.19968920e+00 -1.18198943e-01 ... -2.90152657e-01\n",
      "  -2.00506747e+00 -2.30567508e+00]\n",
      " [-4.71396057e+00 -8.16036413e-01 -1.46922398e+00 ... -9.15080000e-01\n",
      "  -3.15628499e+00  3.32769098e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe9kKR3J-uUA"
   },
   "source": [
    "## <font color = 'blue'> kNN on MNIST attributes from PCA </font>\n",
    "\n",
    "\n",
    "Having calculated the *transformed* MNIST data set I can now apply a kNN approach to the MNIST classification data set. Here are the sets:\n",
    "\n",
    "(i) Fit a $k$-NN classifier on the transformed data set. Here $k$ is a hyperparameter, and I can experiment with it. Be aware though, that larger $k$ can take more time to fit. \n",
    "\n",
    "(ii) Apply the classifier on the transformed test set. What is the classification accuracy? \n",
    "\n",
    "(iii) A theoretical question: if I skipped all the above steps and I just assigned a **random** label to each test point, what would the classification accuracy be on average?  Does the result (ii) beat the random expectation? \n",
    "\n",
    "(iv) Experiment with different settings of $k$, and other hyperparameters that are described in the scikit-learn manual of the kNN classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5v7Q2NKp-uUM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', weights='distance')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(i)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                           algorithm='kd_tree'\n",
    "                           #metric='minkowski'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9354\n"
     ]
    }
   ],
   "source": [
    "#(ii)\n",
    "\n",
    "predicted = knn.predict(X_test_transformed)\n",
    "print(knn.score(X_test_transformed,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Random labels: 0.0974\n"
     ]
    }
   ],
   "source": [
    "#(iii)\n",
    "# Define array of random labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "random_labels = np.random.choice(y.unique(),size=len(y_test))\n",
    "# Get the accuracy with random labels\n",
    "print(\"Accuracy with Random labels:\",accuracy_score(random_labels,y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iii)-contd.\n",
    "#Hence accuracy of random lables is very much low.I am getting only 9.74% , \n",
    "#whereas with respect to PCA followed by KNN, I am getting 93.54% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for n_neighbors=10, weights=distance, algorithm=kd_tree 0.9346\n",
      "Accuracy for n_neighbors=10, weights=distance, algorithm=ball_tree 0.9346\n",
      "Accuracy for n_neighbors=10, weights=distance, algorithm=brute 0.9346\n",
      "Accuracy for n_neighbors=10, weights=uniform, algorithm=kd_tree 0.9328\n",
      "Accuracy for n_neighbors=10, weights=uniform, algorithm=ball_tree 0.9328\n",
      "Accuracy for n_neighbors=10, weights=uniform, algorithm=brute 0.9328\n",
      "Accuracy for n_neighbors=5, weights=uniform, algorithm=kd_tree 0.9347\n",
      "Accuracy for n_neighbors=15, weights=uniform, algorithm=ball_tree 0.9313\n",
      "Accuracy for n_neighbors=20, weights=uniform, algorithm=brute 0.9287\n",
      "Accuracy for n_neighbors=5, weights=distance, algorithm=kd_tree 0.9354\n",
      "Accuracy for n_neighbors=15, weights=distance, algorithm=ball_tree 0.9327\n",
      "Accuracy for n_neighbors=20, weights=distance, algorithm=brute 0.9313\n"
     ]
    }
   ],
   "source": [
    "#(iv)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                           algorithm='kd_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted1 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                           algorithm='ball_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted2 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                            algorithm='brute'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted3 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                           #p=2, \n",
    "                           weights='uniform',\n",
    "                           algorithm='kd_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted4 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                           #p=2, \n",
    "                           weights='uniform',\n",
    "                           algorithm='ball_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted5 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                           #p=2, \n",
    "                           weights='uniform',\n",
    "                            algorithm='brute'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted6 = knn.predict(X_test_transformed)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           #p=2, \n",
    "                           weights='uniform',\n",
    "                           algorithm='kd_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted7 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15, \n",
    "                           #p=2, \n",
    "                           weights='uniform',\n",
    "                           algorithm='ball_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted8 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=20, \n",
    "                           #p=2, \n",
    "                           weights='uniform',\n",
    "                            algorithm='brute'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted9 = knn.predict(X_test_transformed)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                           algorithm='kd_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted10 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                           algorithm='ball_tree'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted11 = knn.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=20, \n",
    "                           #p=2, \n",
    "                           weights='distance',\n",
    "                            algorithm='brute'\n",
    "                          )\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "predicted12 = knn.predict(X_test_transformed)\n",
    "\n",
    "print('Accuracy for n_neighbors=10, weights=distance, algorithm=kd_tree' ,accuracy_score(y_test, predicted1))\n",
    "print('Accuracy for n_neighbors=10, weights=distance, algorithm=ball_tree' ,accuracy_score(y_test, predicted2))\n",
    "print('Accuracy for n_neighbors=10, weights=distance, algorithm=brute' ,accuracy_score(y_test, predicted3))\n",
    "\n",
    "print('Accuracy for n_neighbors=10, weights=uniform, algorithm=kd_tree' ,accuracy_score(y_test, predicted4))\n",
    "print('Accuracy for n_neighbors=10, weights=uniform, algorithm=ball_tree' ,accuracy_score(y_test, predicted5))\n",
    "print('Accuracy for n_neighbors=10, weights=uniform, algorithm=brute' ,accuracy_score(y_test, predicted6))\n",
    "\n",
    "print('Accuracy for n_neighbors=5, weights=uniform, algorithm=kd_tree' ,accuracy_score(y_test, predicted7))\n",
    "print('Accuracy for n_neighbors=15, weights=uniform, algorithm=ball_tree' ,accuracy_score(y_test, predicted8))\n",
    "print('Accuracy for n_neighbors=20, weights=uniform, algorithm=brute' ,accuracy_score(y_test, predicted9))\n",
    "\n",
    "print('Accuracy for n_neighbors=5, weights=distance, algorithm=kd_tree' ,accuracy_score(y_test, predicted10))\n",
    "print('Accuracy for n_neighbors=15, weights=distance, algorithm=ball_tree' ,accuracy_score(y_test, predicted11))\n",
    "print('Accuracy for n_neighbors=20, weights=distance, algorithm=brute' ,accuracy_score(y_test, predicted12))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
